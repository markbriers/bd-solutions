{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 Answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combiner here is exactly the same as the reducer. So combiner.py contains the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "current_key = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        # parse the input we got from mapper.py\n",
    "        key, count = line.split('\\t', 1)\n",
    "\n",
    "        # convert count (currently a string) to int\n",
    "        try:\n",
    "                count = int(count)\n",
    "        except ValueError:\n",
    "                # count was not a number, so silently\n",
    "                # ignore/discard this line\n",
    "                continue\n",
    "\n",
    "        # this IF-switch only works because Hadoop sorts map output\n",
    "        # by key (here: word) before it is passed to the reducer\n",
    "        if current_key == key:\n",
    "                current_count += count\n",
    "        else:\n",
    "                if current_key:\n",
    "                        # write result to STDOUT\n",
    "                        print '%s\\t%s' % (current_key, current_count)\n",
    "                current_count = count\n",
    "                current_key = key\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_key == key:\n",
    "        print '%s\\t%s' % (current_key, current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to execute the Hadoop job (note: USER must be replaced with your username):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_STR/hadoop-streaming-2.7.0-mapr-1808.jar \\\n",
    "-libjars $HADOOP_STR/hadoop-streaming-2.7.0-mapr-1808.jar \\\n",
    "-input textData/* \\\n",
    "-output w2e2-output \\\n",
    "-mapper \"python mapper.py\" \\\n",
    "-file /home/USER/bd-sp-2017/w2e2/mapper.py \\\n",
    "-reducer \"python reducer.py\" \\\n",
    "-file /home/USER/bd-sp-2017/w2e2/reducer.py \\\n",
    "-combiner combiner.py \\\n",
    "-file /home/USER/bd-sp-2017/w2e2/combiner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the mapper is included below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# get input file name\n",
    "input_name = os.environ['mapreduce_map_input_file']\n",
    "\n",
    "# using name of input file figure out if airport is Heathrow or Wick\n",
    "# store this information as a loc = H or loc = W\n",
    "if 'heathrow' in input_name:\n",
    "        loc = 'H'\n",
    "else:\n",
    "        loc = 'W'\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        # if line contains 'yyyy' in it remove it (it is header line)\n",
    "        if 'yyyy' in line:\n",
    "                continue\n",
    "\n",
    "        # split line into year, month, tmax, tmin and rain variables\n",
    "        year, month, tmax, tmin, rain = line.split()\n",
    "\n",
    "        # create key to be 'year-month-loc'\n",
    "        key = '%s-%s-%s' % (year,month,loc)\n",
    "\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited;\n",
    "        print '%s\\t%s\\t%s\\t%s' % (key, tmax, tmin, rain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the reducer is included below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "\n",
    "# print header line\n",
    "print 'yyyy\\tmm\\tHtmax\\tHtmin\\tHrain\\tWtmax\\tWtmin\\tWrain'\n",
    "\n",
    "current_date = None\n",
    "current_loc = None\n",
    "current_output = None\n",
    "date = None\n",
    "loc = None\n",
    "test = 0 # this variable is to keep track if we have come to times when both H and W\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        # parse the input we got from mapper.py\n",
    "        key, tmax, tmin, rain = line.split()\n",
    "\n",
    "        # split key into year, month and location\n",
    "        year, month, loc = key.split('-')\n",
    "\n",
    "        # create date\n",
    "        date = '%s-%s' % (year, month)\n",
    "\n",
    "        # this If-switch only works because Hadoop sorts map output\n",
    "        # by key (here year-month-loc) before it is passed to the reducer\n",
    "        if current_date == date:\n",
    "                # this means data for both airports exists for this date\n",
    "                # write results to STDOUT\n",
    "                print '%s\\t%s\\t' % (year, month) + current_output + '\\t%s\\t%s\\t%s' %(tmax, tmin, rain) \n",
    "\n",
    "                # update current data\n",
    "                current_date = date\n",
    "                current_loc = loc\n",
    "                current_output = '%s\\t%s\\t%s' % (tmax, tmin, rain)\n",
    "\n",
    "                # Set test variable to 1 so that same data is not printed twice\n",
    "                test = 1\n",
    "        else:\n",
    "        # this means that the new date is different from previous\n",
    "        # need to figure out if this is a change from W to W or from W to H\n",
    "                if current_loc == loc:\n",
    "                        # this means we have a change from W to W i.e. no H data is present\n",
    "                        # write results to STDOUT\n",
    "                        print '%s\\t%s\\t-\\t-\\t-\\t' % (current_date.split('-')[0], current_date.split('-')[1]) + current_output\n",
    "\n",
    "                        # update current data\n",
    "                        current_loc = loc\n",
    "                        current_date = date\n",
    "                        current_output = '%s\\t%s\\t%s' % (tmax, tmin, rain)\n",
    "                else:\n",
    "                        # current_loc is empty so set up current data\n",
    "                        current_loc = loc\n",
    "                        current_date = date\n",
    "                        current_output = '%s\\t%s\\t%s' % (tmax, tmin, rain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to execute the Hadoop job (note: USER must be replaced with your username):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_STR/hadoop-streaming-2.7.0-mapr-1808.jar \\\n",
    "-libjars $HADOOP_STR/hadoop-streaming-2.7.0-mapr-1808.jar \\\n",
    "-input temperatureData/* \\\n",
    "-output w2e3-output \\\n",
    "-mapper \"python mapper.py\" \\\n",
    "-file /home/USER/bd-sp-2017/w2e3/mapper.py \\\n",
    "-reducer \"python reducer.py\" \\\n",
    "-file /home/USER/bd-sp-2017/w2e3/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the mapper is included below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "doneFirst = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "\n",
    "        # ignore the first line of data (the header line)\n",
    "        if doneFirst == 0:\n",
    "                doneFirst = 1\n",
    "                continue\n",
    "\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "        # split the line by empty space\n",
    "        cols = line.split(\"\\t\")\n",
    "\n",
    "        # produce the key and value\n",
    "        #key = cols[1] + '\\t' + cols[2]\n",
    "        #value = cols[0] + '\\t' + cols[3] + '\\t' + cols[4]\n",
    "        key = cols[1]\n",
    "        value = '%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s' % (cols[0],cols[2],cols[3],cols[4],cols[5],cols[6],cols[7])\n",
    "\n",
    "        # now output the key and value\n",
    "        print '%s\\t%s' % (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the reducer is included below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "#last_group = None\n",
    "# input comes from STDIN\n",
    "#for line in sys.stdin:\n",
    "        # remove leading and trailing whitespace\n",
    "        #val = line.strip()\n",
    "\n",
    "        #(month,temp) = val.split(\",\")\n",
    "        #group = month\n",
    "        #if last_group != group:\n",
    "                #print val\n",
    "                #last_group = group\n",
    "\n",
    "# print header line\n",
    "print 'yyyy\\tmm\\tHtmax\\tHtmin\\tHrain\\tWtmax\\tWtmin\\tWrain'\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        # parse the input we got from mapper.py\n",
    "        month, year, htmax, htmin, hrain, wtmax, wtmin, wrain = line.split()\n",
    "\n",
    "        # if month is not September continue\n",
    "        if month != '9':\n",
    "                continue\n",
    "\n",
    "        # month is September so print data to STDOUT\n",
    "        print '%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s' % (year, month, htmax, htmin, hrain, wtmax, wtmin, wrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to execute the Hadoop job (note: USER must be replaced with your username):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_STR/hadoop-streaming-2.7.0-mapr-1808.jar \\\n",
    "-libjars $HADOOP_STR/hadoop-streaming-2.7.0-mapr-1808.jar \\\n",
    "-input w2e3-output/part-00000 \\\n",
    "-output w2e4-output \\\n",
    "-mapper \"python mapper.py\" \\\n",
    "-file /home/USER/bd-sp-2017/w2e4/mapper.py \\\n",
    "-reducer \"python reducer.py\" \\\n",
    "-file /home/USER/bd-sp-2017/w2e4/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command to get the results from HDFS and place them in the local filesystem is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -get w2e4-output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the name of the results file to output.txt by using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv part-00000 output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start R by typing R at the terminal. The code to get the data into R, tidy it and plot the time series is included below (note: USER should be replaced with your user name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into a dataframe\n",
    "df = read.table('/home/USER/bd-sp-2017/w2e4/output.txt',header = TRUE)\n",
    "\n",
    "# replace '-' and '---' with NA\n",
    "df[df=='-'] <- NA\n",
    "df[df=='---'] <- NA\n",
    "\n",
    "# convert data to numeric (except for year and month which is left as integer)\n",
    "df[,3:8]=as.data.frame(apply(df[,3:8],2,as.numeric))\n",
    "\n",
    "# sort the data by year\n",
    "df <- df[order(df$yyyy),]\n",
    "\n",
    "# create the time series plots\n",
    "par(mfrow=c(2,1))\n",
    "plot(df$yyyy,df[,3],type='l',xlab='year',ylab='tmax',main='Heathrow')\n",
    "grid()\n",
    "plot(df$yyyy,df[,6],type='l',xlab='year',ylab='tmax',main='Wick')\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot should look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"Rplots-1.png\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from these time series that there seems to be some positive correlation between maximum temperature in Heathrow and in Wick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for the mapper is included below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "doneFirst = 0\n",
    "count = 0\n",
    "Sx = 0\n",
    "Sy = 0\n",
    "Sxy = 0\n",
    "Sxx = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "\n",
    "        # ignore the first line of data (the header line)\n",
    "        if doneFirst == 0:\n",
    "                doneFirst = 1\n",
    "                continue\n",
    "\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        # split the line by empty space\n",
    "        cols = line.split('\\t')\n",
    "\n",
    "        # extract x and y from the data\n",
    "        y = cols[2]\n",
    "        x = cols[5]\n",
    "\n",
    "        # convert  x, y (currently a string) to a float\n",
    "        try:\n",
    "                x = float(x)\n",
    "                y = float(y)\n",
    "        except ValueError:\n",
    "                # x or y was not a float (or was missing),\n",
    "                # so silently ignore/discard this line\n",
    "                continue\n",
    "                \n",
    "        # update count, Sx, Sy, Sxy and Sxx\n",
    "        count += 1\n",
    "        Sx += x\n",
    "        Sy += y\n",
    "        Sxy += x*y\n",
    "        Sxx += x*x\n",
    "\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited\n",
    "        print '%s\\t%s\\t%s\\t%s\\t%s' % (count, Sx, Sy, Sxy, Sxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for the reducer is included below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "tot_count = 0\n",
    "tot_Sx = 0\n",
    "tot_Sy = 0\n",
    "tot_Sxy = 0\n",
    "tot_Sxx = 0\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        # parse the input we got from mapper.py\n",
    "        count, Sx, Sy, Sxy, Sxx = line.split()\n",
    "\n",
    "        # convert count, Sx, Sy, Sxy, Sxx to float\n",
    "        try:\n",
    "                count = float(count)\n",
    "                Sx = float(Sx)\n",
    "                Sy = float(Sy)\n",
    "                Sxy = float(Sxy)\n",
    "                Sxx = float(Sxx)\n",
    "        except ValueError:\n",
    "                # at least one of count, Sx, Sy, Sxy\n",
    "                # , Sxx  was not a float, so\n",
    "                # silently ignore/discard this line\n",
    "                continue\n",
    "\n",
    "        # update total values\n",
    "        tot_count += count\n",
    "        tot_Sx += Sx\n",
    "        tot_Sy += Sy\n",
    "        tot_Sxy += Sxy\n",
    "        tot_Sxx += Sxx\n",
    "\n",
    "# compute determinant of matrix\n",
    "det = (tot_count*tot_Sxx) - (tot_Sx*tot_Sx)\n",
    "\n",
    "# compute intercept and gradient by solving the linear system\n",
    "intercept = (tot_Sxx*tot_Sy) - (tot_Sx*tot_Sxy)\n",
    "intercept /= det\n",
    "gradient = (-tot_Sx*tot_Sy) + (tot_count*tot_Sxy)\n",
    "gradient /= det\n",
    "\n",
    "# write result to STDOUT\n",
    "print '%s\\t%s' %(intercept, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to execute the Hadoop job (note: USER must be replaced with your username):    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_STR/hadoop-streaming-2.7.0-mapr-1808.jar \\\n",
    "-libjars $HADOOP_STR/hadoop-streaming-2.7.0-mapr-1808.jar \\\n",
    "-input w2e3-output/part-00000 \\\n",
    "-output w2e5-output \\\n",
    "-mapper \"python mapper.py\" \\\n",
    "-file /home/USER/bd-sp-2017/w2e5/mapper.py \\\n",
    "-reducer \"python reducer.py\" \\\n",
    "-file /home/USER/bd-sp-2017/w2e5/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the results from HDFS locally and save it as output.txt via the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -get w2e5-output/part* output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out the results with cat utility we obtain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-1.00785740023\t1.48965267536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the intercept is around -1.0 and the gradient is around 1.5 (this is positive as expected from the plots of the two time series)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
